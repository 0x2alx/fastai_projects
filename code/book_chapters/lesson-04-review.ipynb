{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle in /opt/conda/lib/python3.10/site-packages (1.6.14)\n",
      "Requirement already satisfied: six>=1.10 in /opt/conda/lib/python3.10/site-packages (from kaggle) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2023.7.22 in /opt/conda/lib/python3.10/site-packages (from kaggle) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.10/site-packages (from kaggle) (2.9.0.post0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from kaggle) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from kaggle) (4.66.4)\n",
      "Requirement already satisfied: python-slugify in /opt/conda/lib/python3.10/site-packages (from kaggle) (8.0.4)\n",
      "Requirement already satisfied: urllib3 in /opt/conda/lib/python3.10/site-packages (from kaggle) (1.26.18)\n",
      "Requirement already satisfied: bleach in /opt/conda/lib/python3.10/site-packages (from kaggle) (6.1.0)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.10/site-packages (from bleach->kaggle) (0.5.1)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /opt/conda/lib/python3.10/site-packages (from python-slugify->kaggle) (1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->kaggle) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->kaggle) (3.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "iskaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_all = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Code Summary</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Book Chapter 10</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text.all import *\n",
    "\n",
    "if execute_all:\n",
    "    path = untar_data(URLs.IMDB) # Download data in ~/.fastai/data/imdb\n",
    "    \n",
    "    ###### LM fine-tuning ######\n",
    "    get_imdb = partial(get_text_files, folders=['train', 'test', 'unsup']) # Partial function that sets default arguments for the get_text_files function\n",
    "    # Create dataloaders with all our movie reviews\n",
    "    dls_lm = DataBlock(\n",
    "        blocks=TextBlock.from_folder(path, is_lm=True),\n",
    "        get_items=get_imdb, splitter=RandomSplitter(0.1)\n",
    "    ).dataloaders(path, path=path, bs=32, seq_len=80)\n",
    "    \n",
    "    # Create learner and fine-tune the language model for 1 cycle (to learn new embeddings)\n",
    "    learn = language_model_learner(\n",
    "        dls_lm, AWD_LSTM, drop_mult=0.3, \n",
    "        metrics=[accuracy, Perplexity()]).to_fp16()\n",
    "    learn.fit_one_cycle(1, 2e-2)\n",
    "    learn.save_encoder('finetuned')\n",
    "    \n",
    "    ###### Classifier fine-tuning ######\n",
    "    # Create a new dataloaders using only labeled data and a category as the target block (for classification)\n",
    "    dls_clas = DataBlock(\n",
    "        blocks=(TextBlock.from_folder(path, vocab=dls_lm.vocab),CategoryBlock), # Passinfg the previously fine-tuned vocabulary: vocab=dls_lm.vocab\n",
    "        get_y = parent_label,\n",
    "        get_items=partial(get_text_files, folders=['train', 'test']), \n",
    "        splitter=GrandparentSplitter(valid_name='test')\n",
    "    ).dataloaders(path, path=path, bs=32, seq_len=72)\n",
    "    \n",
    "    # Create learner, and load the previously fine-tuned encoder into it\n",
    "    learn_classifier = text_classifier_learner(dls_clas, AWD_LSTM, drop_mult=0.5, \n",
    "                                    metrics=accuracy).to_fp16()\n",
    "    learn_classifier = learn_classifier.load_encoder('finetuned')\n",
    "    \n",
    "    # Train with discriminative learning rates and gradual unfreezing\n",
    "    learn.fit_one_cycle(1, 2e-2) # Most of the layers (except last one) are frozen by default by fastai when using a pre-trained model\n",
    "    learn.freeze_to(-2) # Keep all the layers frozen, except for the last 2\n",
    "    learn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2))\n",
    "    learn.freeze_to(-3) # Keep all the layers frozen, except for the last 3\n",
    "    learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3))\n",
    "    learn.unfreeze() # Unfreeze the whole model\n",
    "    learn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>NB (Getting started with NLP for absolute beginners)</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if execute_all:\n",
    "    from pathlib import Path\n",
    "    import zipfile,kaggle\n",
    "    import pandas as pd\n",
    "    from datasets import Dataset,DatasetDict\n",
    "    from transformers import AutoModelForSequenceClassification,AutoTokenizer\n",
    "    from transformers import TrainingArguments,Trainer\n",
    "    import numpy as np\n",
    "    import datasets\n",
    "    ###### DATA PREP ######\n",
    "    # Setup Kaggle and download data \n",
    "    creds = ''\n",
    "    cred_path = Path('~/.kaggle/kaggle.json').expanduser()\n",
    "    if not cred_path.exists():\n",
    "        cred_path.parent.mkdir(exist_ok=True)\n",
    "        cred_path.write_text(creds)\n",
    "        cred_path.chmod(0o600)\n",
    "    path = Path('us-patent-phrase-to-phrase-matching')\n",
    "    if not iskaggle and not path.exists():\n",
    "        kaggle.api.competition_download_cli(str(path))\n",
    "        zipfile.ZipFile(f'{path}.zip').extractall(path)\n",
    "    if iskaggle:\n",
    "        path = Path('../input/us-patent-phrase-to-phrase-matching')\n",
    "        ! pip install -q datasets\n",
    "    df = pd.read_csv(path/'train.csv')\n",
    "    \n",
    "    # Create the 'input' col\n",
    "    df['input'] = 'TEXT1: ' + df.context + '; TEXT2: ' + df.target + '; ANC1: ' + df.anchor\n",
    "    ds = Dataset.from_pandas(df)\n",
    "    \n",
    "    # Download model and tokenizer\n",
    "    model_nm = 'microsoft/deberta-v3-small'\n",
    "    tokz = AutoTokenizer.from_pretrained(model_nm)\n",
    "    \n",
    "    # Tokenizer/Numericalizer function\n",
    "    def tok_func(x): \n",
    "        return tokz(x[\"input\"])\n",
    "    \n",
    "    # Adds input_ids column with the numericalized input\n",
    "    tok_ds = ds.map(tok_func, batched=True)\n",
    "    tok_ds = tok_ds.rename_columns({'score':'labels'}) # Rename target column to label\n",
    "    \n",
    "    # Create DataSetDict by splitting the training data into train/validation sets\n",
    "    dds = tok_ds.train_test_split(0.25, seed=42)\n",
    "    \n",
    "    # Load and prepare the \"test\" separate dataset for the submission\n",
    "    eval_df = pd.read_csv(path/'test.csv')\n",
    "    eval_df['input'] = 'TEXT1: ' + eval_df.context + '; TEXT2: ' + eval_df.target + '; ANC1: ' + eval_df.anchor\n",
    "    eval_ds = Dataset.from_pandas(eval_df).map(tok_func, batched=True)\n",
    "    \n",
    "    \n",
    "    ###### DEFINE METRICS/LOSS ######\n",
    "    # Utility function to return correlation coefficient between two variables\n",
    "    def corr(x,y): \n",
    "        return np.corrcoef(x,y)[0][1]\n",
    "        \n",
    "    def corr_d(eval_pred): \n",
    "        return {'pearson': corr(*eval_pred)}\n",
    "    \n",
    "    \n",
    "    ###### TRAIN MODEL ######\n",
    "    \n",
    "    # Define hyperparameters\n",
    "    bs = 16\n",
    "    epochs = 4\n",
    "    lr = 8e-5\n",
    "    \n",
    "    # Create a TrainingArguments object for the trainer\n",
    "    args = TrainingArguments('outputs', learning_rate=lr, warmup_ratio=0.1, lr_scheduler_type='cosine', fp16=True,\n",
    "        evaluation_strategy=\"epoch\", per_device_train_batch_size=bs, per_device_eval_batch_size=bs*2,\n",
    "        num_train_epochs=epochs, weight_decay=0.01, report_to='none')\n",
    "    \n",
    "    # Create the model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_nm, num_labels=1)\n",
    "    \n",
    "    # Create the trainer\n",
    "    trainer = Trainer(model, args, train_dataset=dds['train'], eval_dataset=dds['test'],\n",
    "                      tokenizer=tokz, compute_metrics=corr_d)\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "    \n",
    "    \n",
    "    ###### SUBMISSION ######\n",
    "    \n",
    "    # Make predictions on the eval_ds\n",
    "    preds = trainer.predict(eval_ds).predictions.astype(float)\n",
    "    preds = np.clip(preds, 0, 1) # Clip all predicitons to 0 or 1\n",
    "    \n",
    "    submission = datasets.Dataset.from_dict({\n",
    "        'id': eval_ds['id'],\n",
    "        'score': preds\n",
    "    })\n",
    "    #submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Theory Review</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Book Chapter 10</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -Uqq fastbook\n",
    "import fastbook\n",
    "fastbook.setup_book()\n",
    "from fastbook import *\n",
    "from IPython.display import display,HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sheesh! It is amazing how much control the Hollywood establishment has over the entire spectrum of news media. In the morning paper, I read about some new movie for the first time ever. At noon, there it is again in a news magazine I get in the mail. Then I see some \"news\" story about it at six o\\'clock, and later on in the evening there\\'s some story about one of the stars, and later again, an interview with the director and so on. The next day, the movie opens in a theater near you... and it turns out to be one mediocre dog doo of a flick that\\'s begging seats in the \"dollar theatre\" a month later, only to be forgotten by year\\'s end.<br /><br />Then, there are movies like this one. <br /><br />I\\'d never heard of it when I happened by chance to see it at a friend\\'s house. <br /><br />And I\\'ll never forget it. What a masterpiece!<br /><br />If you\\'re a musician, and especially if your first instrument was a hand-me-down, you might appreciate the peculiar tendency of a musical instrument to absorb and even accumulate the human soul, and find its way into the most appropriate hands. That\\'s what this movie is about. Although if you\\'re like me, you might think it\\'s about that one hand-me-down that nobody else wanted, that got you started, the one that years later, when you saw some kid admiring it, you just *knew* it no longer belonged to you...<br /><br />'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the IMDB movie reviews dataset\n",
    "from fastai.text.all import *\n",
    "path = untar_data(URLs.IMDB) # Download data in ~/.fastai/data/imdb\n",
    "files = get_text_files(path, folders = ['train', 'test', 'unsup']) # Create list of lal text files in those 3 folders\n",
    "txt = files[0].open().read() # Get a sample review from the first file\n",
    "files[1].open().read() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language model is model trained to predict the next word, based on past ones. They are trained with self-supervised learning, which means they create the label/targets automatically from the input/training data. Self-supervised learning usually used during the pre-training of the language models (not during transfer learning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>ULMFit</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Universal Language Model Fine-tuning approach improves the performance of a model when using transfer learning, by fine-tuning the sequence-based pretrained language model on the corpus that it will actually be used on, before fine-tuning the classification model itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Text Processing</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind a (next-word predictor) language model is treat text input as a big categorical variable where:\n",
    " - We make list of all possible levels (all words in our training texts)\n",
    " - Replace each level with its vocab index\n",
    " - Create an embedding matrix associated to the vocab\n",
    " - Use the embedding matrix as the first layer of the NN\n",
    "\n",
    "The language model fine-tuning/training is done by taking all the input documents/texts and concatenating them all end to end into a single giant document which wil become the input, and the output/target will be that same giant text btu shifted right by one word. \n",
    "\n",
    "So the language model's final vocab and embedding vectors will consist of the vocabulary learned during its pretraining PLUS the new vocabulary learned during the language-model fine-tuning phase, this wil be language specific to our corpus that the model had not seen before. So the new vocab will combine all those tokens.\n",
    "\n",
    "Necessary Steps:\n",
    " - Tokenization (convert text into character/substrings/word tokens\n",
    " - Numericalization (create vocab list used for looking up token and their ids)\n",
    " - Create LM dataloader (traning data)\n",
    " - Train LM\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Tokenization</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FastAI provides consistent interface to range of external (lib) tokenizers.\n",
    "\n",
    "The WordTokenizer() is always pointing to the current default fastai tokenizer. Ex:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(#143) ['Jiang','Xian','uses','the','complex','backstory','of','Ling','Ling','and','Mao','Daobing','to','study','Mao',\"'s\",'\"','cultural','revolution','\"','(','1966','-','1976',')','at','the','village','level','.'...]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(#5) ['The','U.S.','dollar','1.00','.']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy = WordTokenizer()\n",
    "toks = first(spacy([txt]))\n",
    "print(coll_repr(toks, 30))\n",
    "first(spacy(['The U.S. dollar 1.00.']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also wrap the WordTokenizer() into a FastAI Tokenizer() object which provides extra functionality. It adds extra special tokens (marked by an xx suffix), like xxbos for begining of text/stream or xxmaj to indicate a capitalized word. These rules are meant to make it easier for the model to recognize important aspects of a sentence and to reduce the total vocabulary size by using special tokens to represent repeated characters or capitalized words (instead of maintaing a vocab entry for multiple repetitions or both the lower and upper case version of the same token). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(#158) ['xxbos','xxmaj','jiang','xxmaj','xian','uses','the','complex','backstory','of','xxmaj','ling','xxmaj','ling','and','xxmaj','mao','xxmaj','daobing','to','study','xxmaj','mao',\"'s\",'\"','cultural','revolution','\"','(','1966','-'...]\n"
     ]
    }
   ],
   "source": [
    "tkn = Tokenizer(spacy)\n",
    "print(coll_repr(tkn(txt), 31))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the rules used by Tokenizer() object and their function:\n",
    "\n",
    "- fix_html:: Replaces special HTML characters with a readable version (IMDb reviews have quite a few of these)\n",
    "- replace_rep:: Replaces any character repeated three times or more with a special token for repetition (xxrep), the number of times it's repeated, then the character\n",
    "- replace_wrep:: Replaces any word repeated three times or more with a special token for word repetition (xxwrep), the number of times it's repeated, then the word\n",
    "- spec_add_spaces:: Adds spaces around / and #\n",
    "- rm_useless_spaces:: Removes all repetitions of the space character\n",
    "- replace_all_caps:: Lowercases a word written in all caps and adds a special token for all caps (xxup) in front of it\n",
    "- replace_maj:: Lowercases a capitalized word and adds a special token for capitalized (xxmaj) in front of it\n",
    "- lowercase:: Lowercases all text and adds a special token at the beginning (xxbos) and/or the end (xxeos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<function fastai.text.core.fix_html(x)>,\n",
       " <function fastai.text.core.replace_rep(t)>,\n",
       " <function fastai.text.core.replace_wrep(t)>,\n",
       " <function fastai.text.core.spec_add_spaces(t)>,\n",
       " <function fastai.text.core.rm_useless_spaces(t)>,\n",
       " <function fastai.text.core.replace_all_caps(t)>,\n",
       " <function fastai.text.core.replace_maj(t)>,\n",
       " <function fastai.text.core.lowercase(t, add_bos=True, add_eos=False)>]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "defaults.text_proc_rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Subword Tokenization</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work tokenization assumes that the language has a concept of words and that they are separated by spaces, which is not always the case (like for Chinese, Japanses, Tourkish, etc). To handle those types of languages, subwords tokenization might be used.\n",
    "\n",
    "The idea is to analyze the corpus of documents and create a vocab from the group/sequence of letters that occur most frequently.\n",
    "This means we can control the size of the vocab we want:\n",
    "  - Smaller Tokens (ex characters) = Smaller Vocabulary (only one entry for each character) ==> Slower training and inference because each input requires one token per character, so more tokens for a given sentence, and more computation for inference but lower memory requirements (smaller embedding matrix/vocab).\n",
    "  - Bigger Tokens (ex words/subwords based on frequency) = Bigger vocabulary (there are many ways to combine characters together) ==> Inference is faster since a sentence can be represented with less tokens (because the tokens represent words or subwords and not individual characters) but requires more memory (much bigger matrix embeddings) and much more data for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, subword tokenization provides a way to easily scale between character tokenization (i.e., using a small subword vocab) and word tokenization (i.e., using a large subword vocab), and handles every human language without needing language-specific algorithms to be developed. It can even handle other \"languages\" such as genomic sequences or MIDI music notation! For this reason, in the last year its popularity has soared, and it seems likely to become the most common tokenization approach (it may well already be, by the time you read this!).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Numericalization with fastai</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially the same thing as creating a categorical variable; make a list of all the possible unique levels (tokens) and assign an int index to each of them. This list is then used during the forward/inference pass to convert an input from a list of tokens to a list of integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"(#2152) ['xxunk','xxpad','xxbos','xxeos','xxfld','xxrep','xxwrep','xxup','xxmaj','the',',','.','and','a','of','to','is','in','it','i'...]\",\n",
       " TensorText([   0,    0, 1269,    9, 1270,    0,   14,    0,    0,   12,    0,    0,   15, 1271,    0,   22,   24,    0,  795,   24]),\n",
       " 'xxunk xxunk uses the complex xxunk of xxunk xxunk and xxunk xxunk to study xxunk \\'s \" xxunk revolution \"')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txts = L(o.open().read() for o in files[:2000]) # Create list of strings where each one if a review read from one of the first 2000 files\n",
    "toks200 = txts[:200].map(tkn) # Use a subset of 200 of those reviews\n",
    "num = Numericalize() # Initialize numericalizer. Defaults min_freq=3, max_vocab=60000\n",
    "num.setup(toks200) # Call to setup creates the vocab\n",
    "coll_repr(num.vocab,20), num(toks)[:20], ' '.join(num.vocab[o] for o in num(toks)[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i><b>min_freq=3</b> means that it will not add to the vocabulary any word that appears less than min_freq times in our whole corpus (training texts) and at the same time <b>max_vocab=60000</b> it will only add to the vocabulary the max_vocab most frequent tokens. All tokens less than min_freq or not in the first max_vocab are replaced (by fastai) with xxunk </i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Batching</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>xxbos</td>\n",
       "      <td>xxmaj</td>\n",
       "      <td>in</td>\n",
       "      <td>this</td>\n",
       "      <td>chapter</td>\n",
       "      <td>,</td>\n",
       "      <td>we</td>\n",
       "      <td>will</td>\n",
       "      <td>go</td>\n",
       "      <td>back</td>\n",
       "      <td>over</td>\n",
       "      <td>the</td>\n",
       "      <td>example</td>\n",
       "      <td>of</td>\n",
       "      <td>classifying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>movie</td>\n",
       "      <td>reviews</td>\n",
       "      <td>we</td>\n",
       "      <td>studied</td>\n",
       "      <td>in</td>\n",
       "      <td>chapter</td>\n",
       "      <td>1</td>\n",
       "      <td>and</td>\n",
       "      <td>dig</td>\n",
       "      <td>deeper</td>\n",
       "      <td>under</td>\n",
       "      <td>the</td>\n",
       "      <td>surface</td>\n",
       "      <td>.</td>\n",
       "      <td>xxmaj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>first</td>\n",
       "      <td>we</td>\n",
       "      <td>will</td>\n",
       "      <td>look</td>\n",
       "      <td>at</td>\n",
       "      <td>the</td>\n",
       "      <td>processing</td>\n",
       "      <td>steps</td>\n",
       "      <td>necessary</td>\n",
       "      <td>to</td>\n",
       "      <td>convert</td>\n",
       "      <td>text</td>\n",
       "      <td>into</td>\n",
       "      <td>numbers</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>how</td>\n",
       "      <td>to</td>\n",
       "      <td>customize</td>\n",
       "      <td>it</td>\n",
       "      <td>.</td>\n",
       "      <td>xxmaj</td>\n",
       "      <td>by</td>\n",
       "      <td>doing</td>\n",
       "      <td>this</td>\n",
       "      <td>,</td>\n",
       "      <td>we</td>\n",
       "      <td>'ll</td>\n",
       "      <td>have</td>\n",
       "      <td>another</td>\n",
       "      <td>example</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>of</td>\n",
       "      <td>the</td>\n",
       "      <td>preprocessor</td>\n",
       "      <td>used</td>\n",
       "      <td>in</td>\n",
       "      <td>the</td>\n",
       "      <td>data</td>\n",
       "      <td>block</td>\n",
       "      <td>xxup</td>\n",
       "      <td>api</td>\n",
       "      <td>.</td>\n",
       "      <td>\\n</td>\n",
       "      <td>xxmaj</td>\n",
       "      <td>then</td>\n",
       "      <td>we</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>will</td>\n",
       "      <td>study</td>\n",
       "      <td>how</td>\n",
       "      <td>we</td>\n",
       "      <td>build</td>\n",
       "      <td>a</td>\n",
       "      <td>language</td>\n",
       "      <td>model</td>\n",
       "      <td>and</td>\n",
       "      <td>train</td>\n",
       "      <td>it</td>\n",
       "      <td>for</td>\n",
       "      <td>a</td>\n",
       "      <td>while</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Input text\n",
    "stream = \"In this chapter, we will go back over the example of classifying movie reviews we studied in chapter 1 and dig deeper under the surface. First we will look at the processing steps necessary to convert text into numbers and how to customize it. By doing this, we'll have another example of the PreProcessor used in the data block API.\\nThen we will study how we build a language model and train it for a while.\"\n",
    "tokens = tkn(stream) # Tokenized the text (90 tokens)\n",
    "bs,seq_len = 6,15 # Define batch size (number of streams per batch) and seq_len (number of tokens per sequence) (6x15=90)\n",
    "d_tokens = np.array([tokens[i*seq_len:(i+1)*seq_len] for i in range(bs)]) # Creates 2D matrix (array of arrays) with 6 rows and 15 columns\n",
    "df = pd.DataFrame(d_tokens) # Convert it to a pandas dataframe\n",
    "display(HTML(df.to_html(index=False,header=None)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have 6 streams of 15 tokens that we then subdivide in smaller batches, in this case, seq_len = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>xxbos</td>\n",
       "      <td>xxmaj</td>\n",
       "      <td>in</td>\n",
       "      <td>this</td>\n",
       "      <td>chapter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>movie</td>\n",
       "      <td>reviews</td>\n",
       "      <td>we</td>\n",
       "      <td>studied</td>\n",
       "      <td>in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>first</td>\n",
       "      <td>we</td>\n",
       "      <td>will</td>\n",
       "      <td>look</td>\n",
       "      <td>at</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>how</td>\n",
       "      <td>to</td>\n",
       "      <td>customize</td>\n",
       "      <td>it</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>of</td>\n",
       "      <td>the</td>\n",
       "      <td>preprocessor</td>\n",
       "      <td>used</td>\n",
       "      <td>in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>will</td>\n",
       "      <td>study</td>\n",
       "      <td>how</td>\n",
       "      <td>we</td>\n",
       "      <td>build</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide_input\n",
    "bs,seq_len = 6,5\n",
    "d_tokens = np.array([tokens[i*15:i*15+seq_len] for i in range(bs)])\n",
    "df = pd.DataFrame(d_tokens)\n",
    "display(HTML(df.to_html(index=False,header=None)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>,</td>\n",
       "      <td>we</td>\n",
       "      <td>will</td>\n",
       "      <td>go</td>\n",
       "      <td>back</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>chapter</td>\n",
       "      <td>1</td>\n",
       "      <td>and</td>\n",
       "      <td>dig</td>\n",
       "      <td>deeper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>the</td>\n",
       "      <td>processing</td>\n",
       "      <td>steps</td>\n",
       "      <td>necessary</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxmaj</td>\n",
       "      <td>by</td>\n",
       "      <td>doing</td>\n",
       "      <td>this</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>the</td>\n",
       "      <td>data</td>\n",
       "      <td>block</td>\n",
       "      <td>xxup</td>\n",
       "      <td>api</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>a</td>\n",
       "      <td>language</td>\n",
       "      <td>model</td>\n",
       "      <td>and</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide_input\n",
    "bs,seq_len = 6,5\n",
    "d_tokens = np.array([tokens[i*15+seq_len:i*15+2*seq_len] for i in range(bs)])\n",
    "df = pd.DataFrame(d_tokens)\n",
    "display(HTML(df.to_html(index=False,header=None)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>over</td>\n",
       "      <td>the</td>\n",
       "      <td>example</td>\n",
       "      <td>of</td>\n",
       "      <td>classifying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>under</td>\n",
       "      <td>the</td>\n",
       "      <td>surface</td>\n",
       "      <td>.</td>\n",
       "      <td>xxmaj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>convert</td>\n",
       "      <td>text</td>\n",
       "      <td>into</td>\n",
       "      <td>numbers</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>we</td>\n",
       "      <td>'ll</td>\n",
       "      <td>have</td>\n",
       "      <td>another</td>\n",
       "      <td>example</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>.</td>\n",
       "      <td>\\n</td>\n",
       "      <td>xxmaj</td>\n",
       "      <td>then</td>\n",
       "      <td>we</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>it</td>\n",
       "      <td>for</td>\n",
       "      <td>a</td>\n",
       "      <td>while</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide_input\n",
    "bs,seq_len = 6,5\n",
    "d_tokens = np.array([tokens[i*15+10:i*15+15] for i in range(bs)])\n",
    "df = pd.DataFrame(d_tokens)\n",
    "display(HTML(df.to_html(index=False,header=None)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a larger corpus, like the IMDB movie reviews, at each epoch, we start by shuffling the order of all the text documents (reviews), and then create a mega-stream by concatenating all the reviews together end to end. \n",
    "We divide this stream in a number of fixed-size consecutive mini-streams/batches (called the batch size).\n",
    "We then feed the model mini-batches that contain a part of each of the 10 streams at once, the models keeps an inner state between mini-batches, regardless of the chosen sequence length. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the IMDB movie reviews, we numericalize our toks200 sample, and pass it to LMDataLoader which takes care of splitting the whole corpus into batches and mini-batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 72]), torch.Size([64, 72]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums200 = toks200.map(num)\n",
    "dl = LMDataLoader(nums200)\n",
    "x,y = first(dl)\n",
    "x.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The batch size is 64, so we have 64 mini-streams. \n",
    "- The sequence length is 72 tokens.\n",
    "- There are a total of 14 batches, each containing 64 mini-streams of 72 tokens each (each mini-stream is continuous) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set view_all_batches to print all the rows of all the 14 batches, to visualize how it is split\n",
    "\n",
    "view_all_batches = False\n",
    "\n",
    "if view_all_batches:\n",
    "    batch_index = 0\n",
    "    for batch in dl:\n",
    "        print(\"################## NEW BATCH: ##################\")\n",
    "        print(batch[0].shape)\n",
    "        for row in range(0,64):\n",
    "            print(f\">>>> NEW ROW <<<<\")\n",
    "            print(f\"batch: {batch_index+1} / row: {row+1}\")\n",
    "            print(' '.join(num.vocab[o] for o in batch[0][row]))\n",
    "        batch_index+=1\n",
    "        print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xxbos xxmaj xxunk xxmaj xxunk uses the complex xxunk of xxmaj xxunk xxmaj xxunk and xxmaj xxunk xxmaj xxunk to'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(num.vocab[o] for o in x[0][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xxmaj xxunk xxmaj xxunk uses the complex xxunk of xxmaj xxunk xxmaj xxunk and xxmaj xxunk xxmaj xxunk to study'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(num.vocab[o] for o in y[0][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Training</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization and numericalization handled automatically by the fastai TextBlock when it is passed to a DataBlock. We can pass the same arguments as we do to Tokenize() and Numericalize() above, to TextBlock itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_imdb = partial(get_text_files, folders=['train', 'test', 'unsup']) # Partial function that sets default arguments for the get_text_files function\n",
    "\n",
    "dls_lm = DataBlock(\n",
    "    blocks=TextBlock.from_folder(path, is_lm=True),\n",
    "    get_items=get_imdb, splitter=RandomSplitter(0.1)\n",
    ").dataloaders(path, path=path, bs=32, seq_len=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training/Validation batch size: dls_lm.train.bs=32\n",
      "Sequence length: dls_lm.train.one_batch()[0].shape[1]=80\n",
      "\n",
      "Number of training batches: len(dls_lm.train)=10530\n",
      "Number of validation batches: len(dls_lm.valid)=1161\n",
      "\n",
      "Shape of one training/validaiton batch (input and output): torch.Size([32, 80])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training/Validation batch size: {dls_lm.train.bs=}\")\n",
    "print(f\"Sequence length: {dls_lm.train.one_batch()[0].shape[1]=}\\n\")\n",
    "print(f\"Number of training batches: {len(dls_lm.train)=}\")\n",
    "print(f\"Number of validation batches: {len(dls_lm.valid)=}\\n\")\n",
    "print(f\"Shape of one training/validaiton batch (input and output): {dls_lm.train.one_batch()[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos xxmaj first of all this movie starts out on a really dumb note : a 10 - year - old girl , playing around in a moving vehicle , decides it would be funny to cover up her mom 's eyes with her hands , and then causes a horrific accident which kills the mom … xxunk … .i am sorry , there is positively no 10 - year - old that dumb . xxmaj the rest of the</td>\n",
       "      <td>xxmaj first of all this movie starts out on a really dumb note : a 10 - year - old girl , playing around in a moving vehicle , decides it would be funny to cover up her mom 's eyes with her hands , and then causes a horrific accident which kills the mom … xxunk … .i am sorry , there is positively no 10 - year - old that dumb . xxmaj the rest of the movie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>checking out , and add another star if you 're from xxmaj swansea ! xxbos xxmaj this short film does n't get there . xxmaj cliche ' and not very funny attempt at dark humor . xxmaj humor is n't funny enough to get you interested and the protagonist is n't likeable so you really do n't care about what happens anyway . xxmaj producer spent some money on this flop and it shows in the production value which is</td>\n",
       "      <td>out , and add another star if you 're from xxmaj swansea ! xxbos xxmaj this short film does n't get there . xxmaj cliche ' and not very funny attempt at dark humor . xxmaj humor is n't funny enough to get you interested and the protagonist is n't likeable so you really do n't care about what happens anyway . xxmaj producer spent some money on this flop and it shows in the production value which is the</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls_lm.show_batch(max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Fine-tuning the LM</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to now convert each of the numerlicalized integer intputs into learnable embedding vectors that we can pass through an RNN (Recurrent Neural Network). \n",
    "\n",
    "When we call language_model_learner(), it has a parameter called pretrained with a default value of True, which instructs fastai to create the learner by using a pre-trained model with the architecture AWD_LSTM (fatsai handles the specific model to use in the background).\n",
    "\n",
    "We also pass our dls_lm dataloaders object with our IMDB movie review corpus. The learner will combine the vocabulary (new words/subwords) it sees in the movie review corpus to the pre-trained model's vocabulary. For new tokens, it will create new random embedding vectors and add them to the combined embedding matrix (from pre-trained and fine-tuning corpus)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = language_model_learner(\n",
    "    dls_lm, AWD_LSTM, drop_mult=0.3, \n",
    "    metrics=[accuracy, Perplexity()]).to_fp16()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Loss function: cross-entropy (by default for classificiation)\n",
    "- Accuracy metric: how often predicts next word correctly\n",
    "- Perplexity metric: exponential of cross_entropy (measure of model's confidence in its predictions) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call fit_one_cycle on the learner, so we can save intermediate results (between epochs, which fine_tune doesn't do). By default, when using a pre-trained model, the fastai learner will freeze the pre-trained parameters and only train the new embeddings (the ones that are in the IMDB movie review corpus but were not in the pre-trained model's vocabulary):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.049293</td>\n",
       "      <td>3.954154</td>\n",
       "      <td>0.297404</td>\n",
       "      <td>52.151581</td>\n",
       "      <td>28:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(1, 2e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save the model, use learn.save('1epoch') which will create a file learn.path/models/1epoch.pth.\n",
    "\n",
    "We can then load that model file into a learner with learn.load('1epoch')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('1epoch')\n",
    "\n",
    "learn = learn.load('1epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the new embeddings have been trained, we can unfreeze the rest of the pretrained model and fine-tune all of its parameters, with a lower learning rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if execute_all:\n",
    "    learn.unfreeze()\n",
    "    learn.fit_one_cycle(1, 2e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have finished fine-tuning the (next-word predictor) language model (from the pre-trained one) using our specific corpus (IMDB movie reviews), we can save the encoder of this final fine-tuned model. The encoder is essentially the model without the last layer which is task specific. In this case the last layer has a probability distribution over the entire vocabulary in order to predict the most likely next-word. For a classifier, we want to replace that last layer with one suited for our specific classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save_encoder('finetuned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# How to use a next-word predictor model to generate text:\n",
    "\n",
    "# We provide a seed text (beginning of sentence)\n",
    "TEXT = \"I liked this movie because\"\n",
    "\n",
    "# Specify when to stop generating\n",
    "N_WORDS = 40\n",
    "\n",
    "# Number of generated samples we want \n",
    "N_SENTENCES = 2\n",
    "\n",
    "# Predictions, aka generated texts\n",
    "preds = [learn.predict(TEXT, N_WORDS, temperature=0.75) \n",
    "         for _ in range(N_SENTENCES)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Classifier DataLoaders</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to create a new DataLoader with only the labeled data (we leave out the 'unsup' folder from the IMDB movie review). The validation set is provided as a separate folder, so no need to split up the training data. This dataloader is meant for the classifier model fine-tuning, as opposed to the language model fine-tuning. Some differences:\n",
    "\n",
    "- TextBlock.from_folder doesn't have the is_lm=True parameter, which indicates the dataloader is made of regular labeled data\n",
    "- TextBlock gets passed the vocabulary created previously, so that the vocab and embeddings match "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls_clas = DataBlock(\n",
    "    blocks=(TextBlock.from_folder(path, vocab=dls_lm.vocab),CategoryBlock),\n",
    "    get_y = parent_label,\n",
    "    get_items=partial(get_text_files, folders=['train', 'test']),\n",
    "    splitter=GrandparentSplitter(valid_name='test')\n",
    ").dataloaders(path, path=path, bs=32, seq_len=72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos xxmaj match 1 : xxmaj tag xxmaj team xxmaj table xxmaj match xxmaj bubba xxmaj ray and xxmaj spike xxmaj dudley vs xxmaj eddie xxmaj guerrero and xxmaj chris xxmaj benoit xxmaj bubba xxmaj ray and xxmaj spike xxmaj dudley started things off with a xxmaj tag xxmaj team xxmaj table xxmaj match against xxmaj eddie xxmaj guerrero and xxmaj chris xxmaj benoit . xxmaj according to the rules of the match , both opponents have to go through tables in order to get the win . xxmaj benoit and xxmaj guerrero heated up early on by taking turns hammering first xxmaj spike and then xxmaj bubba xxmaj ray . a xxmaj german xxunk by xxmaj benoit to xxmaj bubba took the wind out of the xxmaj dudley brother . xxmaj spike tried to help his brother , but the referee restrained him while xxmaj benoit and xxmaj guerrero</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xxbos xxmaj some have praised _ xxunk _ as a xxmaj disney adventure for adults . i do n't think so -- at least not for thinking adults . \\n\\n xxmaj this script suggests a beginning as a live - action movie , that struck someone as the type of crap you can not sell to adults anymore . xxmaj the \" crack staff \" of many older adventure movies has been done well before , ( think _ the xxmaj dirty xxmaj dozen _ ) but _ atlantis _ represents one of the worse films in that motif . xxmaj the characters are weak . xxmaj even the background that each member trots out seems stock and awkward at best . xxmaj an xxup md / xxmaj medicine xxmaj man , a tomboy mechanic whose father always wanted sons , if we have not at least seen these before</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xxbos xxmaj warning : xxmaj does contain spoilers . \\n\\n xxmaj open xxmaj your xxmaj eyes \\n\\n xxmaj if you have not seen this film and plan on doing so , just stop reading here and take my word for it . xxmaj you have to see this film . i have seen it four times so far and i still have n't made up my mind as to what exactly happened in the film . xxmaj that is all i am going to say because if you have not seen this film , then stop reading right now . \\n\\n xxmaj if you are still reading then i am going to pose some questions to you and maybe if anyone has any answers you can email me and let me know what you think . \\n\\n i remember my xxmaj grade 11 xxmaj english teacher quite well . xxmaj</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls_clas.show_batch(max_n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a learner with the new data block, then we load the encoder we fine-tuned in the previous section, into the learner object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = text_classifier_learner(dls_clas, AWD_LSTM, drop_mult=0.5, \n",
    "                                metrics=accuracy).to_fp16()\n",
    "\n",
    "learn = learn.load_encoder('finetuned')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Fine-tuning the classifier</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike computer vision models where we train all the layers at once (the model is fully unfrozen for all the training), for NLP, we get better results by using:\n",
    "\n",
    "- Discriminative learning rates (later layers like the classifier use a higher learning rate than early one)\n",
    "- Gradual unfreezing (fine-tune with most layers frozen, and gradually unfreeze more and more layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if execute_all:\n",
    "    learn.fit_one_cycle(1, 2e-2) # Most of the layers (except last one) are frozen by default by fastai when using a pre-trained model\n",
    "    learn.freeze_to(-2) # Keep all the layers frozen, except for the last 2\n",
    "    learn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2))\n",
    "    learn.freeze_to(-3) # Keep all the layers frozen, except for the last 3\n",
    "    learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3))\n",
    "    learn.unfreeze() # Unfreeze the whole model\n",
    "    learn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Questinnaire</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What is \"self-supervised learning\"?\n",
    "\n",
    "  <i>It's a technique for training language models where the target/label is automatically derived from the input data (text) by shifting it.</i>\n",
    "\n",
    "- What is a \"language model\"?\n",
    "\n",
    "    <i>A LM is a model trained to predict the next word, based on past words (seed phrase)</i>\n",
    "\n",
    "- Why is a language model considered self-supervised?\n",
    "\n",
    "    <i>Because it does not require labeled data for training, it creates the labels automatically from the input text</i>\n",
    "\n",
    "- What are self-supervised models usually used for?\n",
    "\n",
    "    <i>They are mostly used as pre-trained model to be fine-tuned for other specific tasks</i>\n",
    "  \n",
    "- Why do we fine-tune language models?\n",
    "\n",
    "    <i>Because they are often trained on a general corpus of text. By fine-tuning it to our specific corpus, it allows the LM to learn additional words/embeddings that were not present in the original texts</i>\n",
    "\n",
    "- What are the three steps to create a state-of-the-art text classifier?\n",
    "\n",
    "    <i>Use or train a language model on a huge data set of english documents. Then fine-tune the language model to our specific corpus. Finaly, replace that LM's last layer with our classification specific layer(s) and fine-tune the classifier.</i>\n",
    "\n",
    "- How do the 50,000 unlabeled movie reviews help us create a better text classifier for the IMDb dataset?\n",
    "\n",
    "    <i>They can be used for fine-tuning the language model (with self-supervised learning)</i>\n",
    "\n",
    "- What are the three steps to prepare your data for a language model?\n",
    "\n",
    "    <i>Tokenization, numericalization and batching of the text</i>\n",
    "  \n",
    "- What is \"tokenization\"? Why do we need it?\n",
    "\n",
    "    <i>It's the process where we convert English words into tokens, that can be either words, subwords or characters, that will eventually make up the model's vocabulary</i>\n",
    "  \n",
    "- Name three different approaches to tokenization.\n",
    "\n",
    "    <i>Word based, sub-word based and character based</i>\n",
    "\n",
    "- What is xxbos?\n",
    "\n",
    "    <i>Indicates the beginning of a text document (a review)</i>\n",
    "  \n",
    "- List four rules that fastai applies to text during tokenization.\n",
    "\n",
    "    <i>Replaces repeated characters with special tokens, replaces capitalized words/letters with special tokens, lowercases capitalized words, lowercases all caps words </i>\n",
    "\n",
    "- Why are repeated characters replaced with a token showing the number of repetitions and the character that's repeated?\n",
    "\n",
    "    <i>To reduce the vocabulary's size, while still maintaing the information of the repetition</i>\n",
    "\n",
    "- What is \"numericalization\"?\n",
    "\n",
    "    <i>The process of mapping tokens to integers (ids)</i>\n",
    "  \n",
    "- Why might there be words that are replaced with the \"unknown word\" token?\n",
    "\n",
    "    <i>Those are for words that did not get added to the vocab (based on the min_freq and max_vocab parameters)</i>\n",
    "  \n",
    "- With a batch size of 64, the first row of the tensor representing the first batch contains the first 64 tokens for the dataset. What does the second row of that tensor contain? What does the first row of the second batch contain? (Careful—students often get this one wrong! Be sure to check your answer on the book's website.)\n",
    "\n",
    "    <i>With a batch-size of 64, it means each batch has 64 ministreams. Depending on the sequence length, and the length of the actual documents, the second row of that first batch would either contain part of the first review, or parts of the second review (text). The first row of the second batch, would contain thet next 64 tokens following the ones in the first row of the frist batch.</i>\n",
    "  \n",
    "- Why do we need padding for text classification? Why don't we need it for language modeling?\n",
    "\n",
    "    <i>For languaqge modeling, we concatenate all our texts together and then split them in equal sized batches. For classification, we need to associate a variable length input to an output, so we batch inputs with similar lengths together, and padd the smaller ones to match the length of the biggest input in that specific batch.</i>\n",
    "  \n",
    "- What does an embedding matrix for NLP contain? What is its shape?\n",
    "\n",
    "    <i>Its a matrix of shape VOCABxEMBEDDING_SIZE where each row index corresponds to a token in the vocabulary, and contains an learnable embedding vector (often of size 512) that represents the meaning of a given token</i>\n",
    "  \n",
    "- What is \"perplexity\"?\n",
    "\n",
    "    <i>The exponential of the cross_entropy</i>\n",
    "  \n",
    "- Why do we have to pass the vocabulary of the language model to the classifier data block?\n",
    "\n",
    "    <i>To make sure we use that same token indexes that were used/learned for the LM fine-tuning</i>\n",
    "  \n",
    "- What is \"gradual unfreezing\"?\n",
    "\n",
    "    <i>To train a model by starting with most of the layers frozen (untrainable) and gradually unfreezing more and more layers at each epoch.</i>\n",
    "  \n",
    "- Why is text generation always likely to be ahead of automatic identification of machine-generated texts?\n",
    "\n",
    "    <i>Because the models used for automatic indentification of machine-generated texts can also be used to fine-tune those models further and make them harder to detect.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>NB (Getting started with NLP for absolute beginners)</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creds = ''\n",
    "cred_path = Path('~/.kaggle/kaggle.json').expanduser()\n",
    "if not cred_path.exists():\n",
    "    cred_path.parent.mkdir(exist_ok=True)\n",
    "    cred_path.write_text(creds)\n",
    "    cred_path.chmod(0o600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('us-patent-phrase-to-phrase-matching')\n",
    "if not iskaggle and not path.exists():\n",
    "    kaggle.api.competition_download_cli(str(path))\n",
    "    zipfile.ZipFile(f'{path}.zip').extractall(path)\n",
    "if iskaggle:\n",
    "    path = Path('../input/us-patent-phrase-to-phrase-matching')\n",
    "    ! pip install -q datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_submission.csv  test.csv  train.csv\n"
     ]
    }
   ],
   "source": [
    "!ls {path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>anchor</th>\n",
       "      <th>target</th>\n",
       "      <th>context</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37d61fd2272659b1</td>\n",
       "      <td>abatement</td>\n",
       "      <td>abatement of pollution</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7b9652b17b68b7a4</td>\n",
       "      <td>abatement</td>\n",
       "      <td>act of abating</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36d72442aefd8232</td>\n",
       "      <td>abatement</td>\n",
       "      <td>active catalyst</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5296b0c19e1ce60e</td>\n",
       "      <td>abatement</td>\n",
       "      <td>eliminating process</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54c1e3b9184cb5b6</td>\n",
       "      <td>abatement</td>\n",
       "      <td>forest region</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36468</th>\n",
       "      <td>8e1386cbefd7f245</td>\n",
       "      <td>wood article</td>\n",
       "      <td>wooden article</td>\n",
       "      <td>B44</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36469</th>\n",
       "      <td>42d9e032d1cd3242</td>\n",
       "      <td>wood article</td>\n",
       "      <td>wooden box</td>\n",
       "      <td>B44</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36470</th>\n",
       "      <td>208654ccb9e14fa3</td>\n",
       "      <td>wood article</td>\n",
       "      <td>wooden handle</td>\n",
       "      <td>B44</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36471</th>\n",
       "      <td>756ec035e694722b</td>\n",
       "      <td>wood article</td>\n",
       "      <td>wooden material</td>\n",
       "      <td>B44</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36472</th>\n",
       "      <td>8d135da0b55b8c88</td>\n",
       "      <td>wood article</td>\n",
       "      <td>wooden substrate</td>\n",
       "      <td>B44</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36473 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id        anchor                  target context  score\n",
       "0      37d61fd2272659b1     abatement  abatement of pollution     A47   0.50\n",
       "1      7b9652b17b68b7a4     abatement          act of abating     A47   0.75\n",
       "2      36d72442aefd8232     abatement         active catalyst     A47   0.25\n",
       "3      5296b0c19e1ce60e     abatement     eliminating process     A47   0.50\n",
       "4      54c1e3b9184cb5b6     abatement           forest region     A47   0.00\n",
       "...                 ...           ...                     ...     ...    ...\n",
       "36468  8e1386cbefd7f245  wood article          wooden article     B44   1.00\n",
       "36469  42d9e032d1cd3242  wood article              wooden box     B44   0.50\n",
       "36470  208654ccb9e14fa3  wood article           wooden handle     B44   0.50\n",
       "36471  756ec035e694722b  wood article         wooden material     B44   0.75\n",
       "36472  8d135da0b55b8c88  wood article        wooden substrate     B44   0.50\n",
       "\n",
       "[36473 rows x 5 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(path/'train.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>anchor</th>\n",
       "      <th>target</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>36473</td>\n",
       "      <td>36473</td>\n",
       "      <td>36473</td>\n",
       "      <td>36473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>36473</td>\n",
       "      <td>733</td>\n",
       "      <td>29340</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>8d135da0b55b8c88</td>\n",
       "      <td>component composite coating</td>\n",
       "      <td>composition</td>\n",
       "      <td>H01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>152</td>\n",
       "      <td>24</td>\n",
       "      <td>2186</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                       anchor       target context\n",
       "count              36473                        36473        36473   36473\n",
       "unique             36473                          733        29340     106\n",
       "top     8d135da0b55b8c88  component composite coating  composition     H01\n",
       "freq                   1                          152           24    2186"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe(include='object')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create 'input' column by combining multiple columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['input'] = 'TEXT1: ' + df.context + '; TEXT2: ' + df.target + '; ANC1: ' + df.anchor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    TEXT1: A47; TEXT2: abatement of pollution; ANC1: abatement\n",
       "1            TEXT1: A47; TEXT2: act of abating; ANC1: abatement\n",
       "2           TEXT1: A47; TEXT2: active catalyst; ANC1: abatement\n",
       "3       TEXT1: A47; TEXT2: eliminating process; ANC1: abatement\n",
       "4             TEXT1: A47; TEXT2: forest region; ANC1: abatement\n",
       "Name: input, dtype: object"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.input.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Tokenization</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'anchor', 'target', 'context', 'score', 'input'],\n",
       "    num_rows: 36473\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset,DatasetDict\n",
    "ds = Dataset.from_pandas(df)\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to download a model to use its tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3327cd734c849d1a43a3f3d2083ae45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcb4d4e8ab484211a368018fcb1d6618",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/578 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22d9ce52218a427aae9f04ea9673b28f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['▁A',\n",
       " '▁platypus',\n",
       " '▁is',\n",
       " '▁an',\n",
       " '▁or',\n",
       " 'ni',\n",
       " 'tho',\n",
       " 'rhynch',\n",
       " 'us',\n",
       " '▁an',\n",
       " 'at',\n",
       " 'inus',\n",
       " '.']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification,AutoTokenizer\n",
    "\n",
    "model_nm = 'microsoft/deberta-v3-small'\n",
    "tokz = AutoTokenizer.from_pretrained(model_nm)\n",
    "tokz.tokenize(\"A platypus is an ornithorhynchus anatinus.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that tokenizes the 'input' column for each data sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tok_func(x): \n",
    "    return tokz(x[\"input\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the tok_func to all the rows in our dataset (ds), creates a new column, input_ids, which is the tokenized and numericalized version of 'input'. The tokenizer contains an indexed list of all string tokens in tokz.vocab, which is used to get the numerical ID of each token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db0f9903505e41ab9990e1d36b9bfb0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tok_ds = ds.map(tok_func, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '37d61fd2272659b1',\n",
       " 'anchor': 'abatement',\n",
       " 'target': 'abatement of pollution',\n",
       " 'context': 'A47',\n",
       " 'score': 0.5,\n",
       " 'input': 'TEXT1: A47; TEXT2: abatement of pollution; ANC1: abatement',\n",
       " 'input_ids': [1,\n",
       "  54453,\n",
       "  435,\n",
       "  294,\n",
       "  336,\n",
       "  5753,\n",
       "  346,\n",
       "  54453,\n",
       "  445,\n",
       "  294,\n",
       "  47284,\n",
       "  265,\n",
       "  6435,\n",
       "  346,\n",
       "  23702,\n",
       "  435,\n",
       "  294,\n",
       "  47284,\n",
       "  2],\n",
       " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_ds = tok_ds.rename_columns({'score':'labels'}) #To conform with Transformers lib expected 'label' target column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Test and Validation sets</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the validation, set, we can define it in a DatasetDict (object that contains multiple DataSet objects) by splitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'anchor', 'target', 'context', 'labels', 'input', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 27354\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'anchor', 'target', 'context', 'labels', 'input', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 9119\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dds = tok_ds.train_test_split(0.25, seed=42)\n",
    "dds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test set provided as a separate file and it is to be used at the very end, after trying multiple models and settling on a final one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a772b2bf480435eb8839b5f25822973",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>anchor</th>\n",
       "      <th>target</th>\n",
       "      <th>context</th>\n",
       "      <th>input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>36</td>\n",
       "      <td>34</td>\n",
       "      <td>36</td>\n",
       "      <td>29</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>4112d61851461f60</td>\n",
       "      <td>hybrid bearing</td>\n",
       "      <td>inorganic photoconductor drum</td>\n",
       "      <td>G02</td>\n",
       "      <td>TEXT1: G02; TEXT2: inorganic photoconductor drum; ANC1: opc drum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id          anchor                         target  \\\n",
       "count                 36              36                             36   \n",
       "unique                36              34                             36   \n",
       "top     4112d61851461f60  hybrid bearing  inorganic photoconductor drum   \n",
       "freq                   1               2                              1   \n",
       "\n",
       "       context  \\\n",
       "count       36   \n",
       "unique      29   \n",
       "top        G02   \n",
       "freq         3   \n",
       "\n",
       "                                                                   input  \n",
       "count                                                                 36  \n",
       "unique                                                                36  \n",
       "top     TEXT1: G02; TEXT2: inorganic photoconductor drum; ANC1: opc drum  \n",
       "freq                                                                   1  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df = pd.read_csv(path/'test.csv')\n",
    "eval_df['input'] = 'TEXT1: ' + eval_df.context + '; TEXT2: ' + eval_df.target + '; ANC1: ' + eval_df.anchor\n",
    "eval_ds = Dataset.from_pandas(eval_df).map(tok_func, batched=True)\n",
    "eval_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Metrics and correlation</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The competition was evaluated on the Pearson correlation coefficient, r, which has a range of -1 to 1 (perfect positive correlation).\n",
    "\n",
    "We define a corr function, which returns the correlation coefficient between two variables (it is returned as a 2x2 matrix). Then the corr_d utility function that simply wraps the returned result in a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr(x,y): \n",
    "    return np.corrcoef(x,y)[0][1]\n",
    "\n",
    "\n",
    "def corr_d(eval_pred): \n",
    "    return {'pearson': corr(*eval_pred)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Training model</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-18 14:59:05.653993: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-18 14:59:05.654138: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-18 14:59:05.833985: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments,Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define hyperparameters and create a TrainingArguments object (required for transformers):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "bs = 16\n",
    "epochs = 4\n",
    "lr = 8e-5\n",
    "\n",
    "args = TrainingArguments('outputs', learning_rate=lr, warmup_ratio=0.1, lr_scheduler_type='cosine', fp16=True,\n",
    "    evaluation_strategy=\"epoch\", per_device_train_batch_size=bs, per_device_eval_batch_size=bs*2,\n",
    "    num_train_epochs=epochs, weight_decay=0.01, report_to='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create and train the classification model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1867e550ef1342d5bd64f39e99db26dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/286M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3420' max='3420' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3420/3420 15:51, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Pearson</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.048400</td>\n",
       "      <td>0.038366</td>\n",
       "      <td>0.772261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.024200</td>\n",
       "      <td>0.025006</td>\n",
       "      <td>0.816900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.014400</td>\n",
       "      <td>0.023314</td>\n",
       "      <td>0.828949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.010900</td>\n",
       "      <td>0.022094</td>\n",
       "      <td>0.832348</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_nm, num_labels=1)\n",
    "trainer = Trainer(model, args, train_dataset=dds['train'], eval_dataset=dds['test'],\n",
    "                  tokenizer=tokz, compute_metrics=corr_d)\n",
    "\n",
    "trainer.train();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Predictions and submission</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions on the eval_ds (the test.csv file) to use for the submission. We use the clip() function to set all values greater than 1 to 1 and all negative values to 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[0.46401793],\n",
       "       [0.67237478],\n",
       "       [0.57941985],\n",
       "       [0.38659182],\n",
       "       [0.        ],\n",
       "       [0.53017342],\n",
       "       [0.51792258],\n",
       "       [0.        ],\n",
       "       [0.26737645],\n",
       "       [1.        ],\n",
       "       [0.19829026],\n",
       "       [0.2516216 ],\n",
       "       [0.69299537],\n",
       "       [0.99349993],\n",
       "       [0.77207237],\n",
       "       [0.41444772],\n",
       "       [0.252572  ],\n",
       "       [0.        ],\n",
       "       [0.57144505],\n",
       "       [0.35935143],\n",
       "       [0.45219156],\n",
       "       [0.21763106],\n",
       "       [0.03066588],\n",
       "       [0.2434327 ],\n",
       "       [0.55008698],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.60079515],\n",
       "       [0.33296514],\n",
       "       [0.        ],\n",
       "       [0.74083984],\n",
       "       [0.56434649],\n",
       "       [0.38907242],\n",
       "       [0.24090996]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = trainer.predict(eval_ds).predictions.astype(float)\n",
    "preds = np.clip(preds, 0, 1)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e65e07c79464068a55e77a6f98411db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'id': '4112d61851461f60', 'score': [0.46401792764663696]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "submission = datasets.Dataset.from_dict({\n",
    "    'id': eval_ds['id'],\n",
    "    'score': preds\n",
    "})\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "submission[0]"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
